{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0__Y6ufpuHS7"
      },
      "source": [
        "# Reproducting Graph Neural Collaborative Filtering\n",
        "\n",
        "[Link to original paper](https://arxiv.org/abs/1905.08108)\n",
        "\n",
        "Authors: \n",
        " - @MathiasMeuleman\n",
        " - @SytzeAndr\n",
        " - @sinuochen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_JxtJyWFH0D",
        "outputId": "e10cf70b-bc92-4b4e-fa08-a49bf645c620"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHUfQ4EzOos4"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch_geometric\n",
        "\n",
        "from pathlib import Path"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GuQIsFhz_O6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5519ed0-b933-4aa0-9d51-ee6613724aaa"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount drive for permanent file storage\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK_0Qlpt_yqp"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0FRqbX_k4O"
      },
      "source": [
        "def split_mtx(X, n_folds=200):\n",
        "  \"\"\"\n",
        "  Split a matrix/Tensor into n parts.\n",
        "  Useful for processing large matrices in batches\n",
        "  \"\"\"\n",
        "  X_folds = []\n",
        "  fold_len = X.shape[0]//n_folds\n",
        "  for i in range(n_folds):\n",
        "    start = i * fold_len\n",
        "    if i == n_folds -1:\n",
        "      end = X.shape[0]\n",
        "    else:\n",
        "      end = (i + 1) * fold_len\n",
        "    X_folds.append(X[start:end])\n",
        "  return X_folds\n",
        "\n",
        "def to_sparse_tensor(X):\n",
        "  \"\"\"\n",
        "  Convert a sparse numpy object to a sparse pytorch tensor.\n",
        "  Note that the tensor does not yet live on the GPU\n",
        "  \"\"\"\n",
        "  coo = X.tocoo().astype(np.float32)\n",
        "  i = torch.LongTensor(np.mat((coo.row, coo.col)))\n",
        "  v = torch.FloatTensor(coo.data)\n",
        "  return torch.sparse.FloatTensor(i, v, coo.shape)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p4cMwaf12hb"
      },
      "source": [
        "# Data Loading\n",
        "\n",
        "The `DataLoader` class mainly takes care of loading the data. In addition, it has functions to compute and store the adjacency matrix and can be used for easy batch sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlgYT5KcKugh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8cbb7a-4a49-4c07-ce5d-6e186f36f294"
      },
      "source": [
        "path = Path('./drive/My Drive/NGCF_data/')\n",
        "train_file = path/'train.txt'\n",
        "test_file = path/'test.txt'\n",
        "\n",
        "class DataLoader:\n",
        "  def __init__(self, file, batch_size):\n",
        "    self.file = file\n",
        "    self.batch_size = batch_size\n",
        "    self.n_users, self.n_items, self.n_data = 0, 0, 0\n",
        "    self.users = []\n",
        "    self.pos_items = {}\n",
        "    self.neg_items = {}\n",
        "    self.load()\n",
        "\n",
        "  def load(self):\n",
        "    with open(self.file) as f:\n",
        "      for l in f.readlines():#每个user买了什么item（in train.txt）\n",
        "        if len(l) == 0: break\n",
        "        l = l.strip('\\n').split(' ')\n",
        "\n",
        "        uid = int(l[0])\n",
        "        try:\n",
        "          items = [int(i) for i in l[1:]]\n",
        "        except Exception:\n",
        "          continue\n",
        "        self.users.append(uid)\n",
        "        self.n_items = max(self.n_items, max(items))#目前取得的最大的 item id\n",
        "        self.n_users = max(self.n_users, uid)   #目前取得的最大的 user id\n",
        "        self.n_data += len(items)          #n_data存所有user\"买\"这个行为的次数\n",
        "        self.pos_items[uid] = items         #记录某个user买了的items，pos_items 是一个二维数组\n",
        "    self.n_users += 1\n",
        "    self.n_items += 1\n",
        "    \n",
        "    # R is the Rating matrix in Dict Of Keys form, either 1. or 0. for each (user, item) pair\n",
        "    self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "    \n",
        "    for u in self.users:\n",
        "      for i in self.pos_items[u]:\n",
        "        self.R[u, i] = 1.\n",
        "    #self.R成为一个shape(29858，40982)的matrix，如果一个user-item pair 存在，为1，不存在则为0\n",
        "    #print(\"self.R\",self.R.get_shape())\n",
        "\n",
        "  def compute_norm_adj_matrix(self, adj):\n",
        "    #axis 0 即返回一行数（每列的和），axis 1 即返回一列（每行的和）   \n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    # inverted and set to 0 if no connections\n",
        "    # rowsum的each element^(-1), 然后让其变为一行\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    # sparse diagonal matrix with the normalizing factors in the diagonal\n",
        "    # Construct a sparse matrix from diagonals.\n",
        "    d_mat_inv = sp.diags(d_inv)\n",
        "    # dot product resulting in a row-normalised version of the input matrix\n",
        "    norm_adj = d_mat_inv.dot(adj)\n",
        "    return norm_adj.tocoo()\n",
        "  \n",
        "  def set_adj_matrix(self):\n",
        "    try:\n",
        "      self.adj_matrix = sp.load_npz(path/'s_adj_mat.npz')\n",
        "      print('Loaded existing adj_matrix')\n",
        "    except Exception:\n",
        "      print('No exisiting adj_matrix found')\n",
        "      adj = self.compute_adj_matrix()\n",
        "      sp.save_npz(path/'s_adj_mat.npz', adj.tocsr())\n",
        "      self.adj_matrix = adj\n",
        "\n",
        "  def compute_adj_matrix(self):\n",
        "    # A is the Adjecency matrix in Dict Of Keys form, used when computing the Laplacian norm\n",
        "    A = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32).tolil()\n",
        "    A[:self.n_users, self.n_users:] = self.R.tolil()  ##返回sparse matrix 的lil_matrix形式Convert this matrix to List of Lists format.\n",
        "    A[self.n_users:, :self.n_users] = self.R.tolil().T\n",
        "    A = A.todok() #返回sparse matrix 的 dok_matrix形式\n",
        "\n",
        "    # norm_adj = self.compute_norm_adj_matrix(A + sp.eye(A.shape[0]))\n",
        "    mean_adj = self.compute_norm_adj_matrix(A)\n",
        "    # L is the Laplacian used for normalizing message construction\n",
        "    # ngcf_adj = mean_adj + sp.eye(mean_adj.shape[0])\n",
        "    self.adj_matrix = mean_adj + sp.eye(mean_adj.shape[0])#sp.eye(num of rows in the matrix) Returns a sparse (m x n) matrix where the kth diagonal is all ones and everything else is zeros\n",
        "    return mean_adj + sp.eye(mean_adj.shape[0])\n",
        " \n",
        "\n",
        "  \"\"\"\n",
        "    For each observed user-item interaction, we treat it as a positive\n",
        "  instance, and then conduct the negative sampling strategy to pair\n",
        "  it with one negative item that the user did not consume before.\n",
        "  \"\"\"\n",
        "  def sample_pos(self, u, amount):\n",
        "    # Sample a batch of <amount> positive items for user u\n",
        "    high = len(self.pos_items[u])\n",
        "    pos_sample = []\n",
        "    while len(pos_sample) < amount:\n",
        "      id = np.random.randint(low=0, high=high, size=1)[0]\n",
        "      item = self.pos_items[u][id]\n",
        "      if item not in pos_sample:\n",
        "        pos_sample.append(item)\n",
        "    return pos_sample\n",
        "\n",
        "  def sample_neg(self, u, amount):\n",
        "    # Sample a batch of <amount> negative items for user u\n",
        "    high = self.n_items\n",
        "    neg_sample = []\n",
        "    while len(neg_sample) < amount:\n",
        "      item = np.random.randint(low=0, high=high, size=1)[0]\n",
        "      if item not in self.pos_items[u] and item not in neg_sample:\n",
        "        neg_sample.append(item)\n",
        "    return neg_sample\n",
        "\n",
        "  def sample(self):\n",
        "    # Sample a batch of batch_size users, each with a positive and negative item\n",
        "    users = np.random.choice(self.users, size=self.batch_size) #随机从0-self.users中选出self.batch_size个整数，变成np array之后传给users\n",
        "    pos_sample, neg_sample = [], []\n",
        "    for u in users:\n",
        "      pos_sample += self.sample_pos(u, 1)\n",
        "      neg_sample += self.sample_neg(u, 1)\n",
        "    \n",
        "    # print(\"users\", users.shape)      #1024\n",
        "    # print(\"pos_sample\", len(pos_sample)) #1024\n",
        "    # print(\"neg_sample\", len(neg_sample)) #1024\n",
        "    return users, pos_sample, neg_sample\n",
        "\n",
        "train_data = DataLoader(train_file, batch_size=1024)\n",
        "train_data.set_adj_matrix()\n",
        "test_data = DataLoader(test_file, batch_size=1024)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing adj_matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZpW3QFz2A7Y"
      },
      "source": [
        "# Model definition\n",
        "\n",
        "The NGCF class defines the network for the NGCF framework. The forward function performs message construction and message aggregation, followed by computing the pairwise BPR loss on the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ueycn8nn2DTD"
      },
      "source": [
        "from torch.nn import init, LeakyReLU, Linear, Module, ModuleList, Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "\n",
        "# path to save model to\n",
        "models_path = path/'models/2layer'\n",
        "\n",
        "class NGCF(Module):\n",
        "  def __init__(self, n_users, n_items, embed_size, n_layers, adj_matrix):\n",
        "    super().__init__()\n",
        "    self.n_users = n_users\n",
        "    self.n_items = n_items\n",
        "    self.embed_size = embed_size\n",
        "    self.n_layers = n_layers\n",
        "    self.adj_matrix = adj_matrix\n",
        "\n",
        "    # The (user/item)_embeddings are the initial embedding matrix E\n",
        "    self.user_embeddings = Parameter(torch.rand(n_users, embed_size))\n",
        "    self.item_embeddings = Parameter(torch.rand(n_items, embed_size))\n",
        "    print(\"\\n self.user_embeddings\", self.user_embeddings.shape)# self.user_embeddings torch.Size([29858, 64])\n",
        "    print(\"\\n self.item_embeddings\", self.item_embeddings.shape)# self.item_embeddings torch.Size([40981, 64])\n",
        "\n",
        "    # The (user/item)_embeddings_final are the final concatenated embeddings [E_1..E_L]\n",
        "    # Stored for easy tracking of final embeddings throughout optimization and eval\n",
        "    self.user_embeddings_final = Parameter(torch.zeros((n_users, embed_size * (n_layers + 1))))#充满zero\n",
        "    self.item_embeddings_final = Parameter(torch.zeros((n_items, embed_size * (n_layers + 1))))\n",
        "    print(\"\\n self.user_embeddings_final\", self.user_embeddings_final.shape)# self.user_embeddings_final torch.Size([29858, 192])\n",
        "    print(\"\\n self.item_embeddings_final\", self.item_embeddings_final.shape)# self.item_embeddings_final torch.Size([40981, 192])\n",
        "\n",
        "    # The linear transformations for each layer\n",
        "    self.W1 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, self.n_layers)]) # y=b+W1^T * X  \n",
        "    self.W2 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, self.n_layers)])\n",
        "    print(\"\\n self.W1\", self.W1)\n",
        "    print(\"\\n self.W2\", self.W2)\n",
        "    \"\"\"\n",
        "          self.W1 ModuleList((0-1): 2 x Linear(in_features=64, out_features=64, bias=True))\n",
        "          self.W2 ModuleList((0-1): 2 x Linear(in_features=64, out_features=64, bias=True))\n",
        "    \"\"\"\n",
        "    \n",
        "    self.act = LeakyReLU()\n",
        "    \n",
        "    # Initialize each of the trainable weights with the Xavier initializer\n",
        "    self.init_weights()\n",
        "\n",
        "  \n",
        "\n",
        "  def init_weights(self):#initialize W1 W2\n",
        "    \n",
        "    for name, parameter in self.named_parameters():\n",
        "      \n",
        "      if ('bias' not in name):\n",
        "        init.xavier_uniform_(parameter)\n",
        "\n",
        "  def compute_loss(self, batch_user_emb, batch_pos_emb, batch_neg_emb):\n",
        "    pos_y = torch.mul(batch_user_emb, batch_pos_emb).sum(dim=1)\n",
        "    neg_y = torch.mul(batch_user_emb, batch_neg_emb).sum(dim=1)\n",
        "    # Unregularized loss\n",
        "    bpr_loss = -(torch.log(torch.sigmoid(pos_y - neg_y))).mean()\n",
        "    return bpr_loss\n",
        "\n",
        "  def message(self, x_i, x_j, norm):\n",
        "    # Construct message\n",
        "    message = self.lin1(x_j)\n",
        "    # Only add the second term if it is not a self loop\n",
        "    if x_i.data_ptr() != x_j.data_ptr():\n",
        "      message += self.lin2(torch.mul(x_i, x_j))\n",
        "    return message\n",
        "\n",
        "  def update(self, aggr_out):\n",
        "    # Return the LeakyReLU result\n",
        "    return self.act(aggr_out)\n",
        "\n",
        "  def forward(self, u, i, j):\n",
        "    #print(\"self.adj_matrix\", self.adj_matrix)\n",
        "    adj_splits = split_mtx(self.adj_matrix)\n",
        "    #print(\"self.adj_splits\", adj_splits)\n",
        "    #the connected user-item pair\n",
        "    embeddings = torch.cat((self.user_embeddings, self.item_embeddings))#E=[e_u1,...,e_uN, e_i1...,e_iM ]\n",
        "    final_embeddings = [embeddings]\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      embedding_parts = []\n",
        "      for part in adj_splits:\n",
        "        embedding_parts.append(torch.sparse.mm(to_sparse_tensor(part).to(device), embeddings))\n",
        "\n",
        "      # Message construction\n",
        "      t1_embeddings = torch.cat(embedding_parts, 0)\n",
        "      t1 = self.W1[l](t1_embeddings)\n",
        "      t2_embeddings = embeddings.mul(t1_embeddings)\n",
        "      t2 = self.W2[l](t2_embeddings)\n",
        "\n",
        "      # Message aggregation\n",
        "      embeddings = self.act(t1 + t2)\n",
        "      normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "      final_embeddings.append(normalized_embeddings)\n",
        "\n",
        "    # Make sure to update the (user/item)_embeddings(_final)\n",
        "    final_embeddings = torch.cat(final_embeddings, 1)\n",
        "    final_u_embeddings, final_i_embeddings = final_embeddings.split((self.n_users, self.n_items), 0)\n",
        "    self.user_embeddings_final = Parameter(final_u_embeddings)\n",
        "    self.item_embeddings_final = Parameter(final_i_embeddings)\n",
        "\n",
        "    batch_user_emb = final_u_embeddings[u]\n",
        "    batch_pos_emb = final_i_embeddings[i]\n",
        "    batch_neg_emb = final_i_embeddings[j]\n",
        "\n",
        "    return self.compute_loss(batch_user_emb, batch_pos_emb, batch_neg_emb)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v49krYQRskMO"
      },
      "source": [
        "Define the model and optimizer. Includes a utility function to restore the state of the model and optimizer. Handy when training for a long time and finding that Google suddenly restricted yout GPU time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3plBLDjm5NH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee24d8e-ca85-466d-9f97-8dc740aff189"
      },
      "source": [
        "# optional: restore previously trained model\n",
        "def restore_model(path, model, optimizer):\n",
        "  model.load_state_dict(torch.load(path/'model1.pt'))\n",
        "  optimizer.load_state_dict(torch.load(path/'optimizer.pt'))\n",
        "  print('Restored previous model')\n",
        "  return model, optimizer\n",
        "\n",
        "# norm_adj, mean_adj, ngcf_adj = train_data.compute_adj_matrix()\n",
        "device = torch.device('cuda')\n",
        "model = NGCF(n_users=train_data.n_users, n_items=train_data.n_items, embed_size=64, n_layers=2, adj_matrix=train_data.adj_matrix).to(device)\n",
        "# XXX = model.named_parameters\n",
        "# print(XXX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005) #We adopt mini-batch Adam [17] to optimize the prediction model and update the model parameters.\n",
        "print((list(model.parameters())))\n",
        "# Restore a previous model\n",
        "model, optimizer = restore_model(models_path, model, optimizer)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " self.user_embeddings torch.Size([29858, 64])\n",
            "\n",
            " self.item_embeddings torch.Size([40981, 64])\n",
            "\n",
            " self.user_embeddings_final torch.Size([29858, 192])\n",
            "\n",
            " self.item_embeddings_final torch.Size([40981, 192])\n",
            "\n",
            " self.W1 ModuleList(\n",
            "  (0-1): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
            ")\n",
            "\n",
            " self.W2 ModuleList(\n",
            "  (0-1): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
            ")\n",
            "[Parameter containing:\n",
            "tensor([[-3.8381e-03, -5.4686e-03, -1.0556e-02,  ...,  2.1115e-03,\n",
            "         -4.6655e-03, -1.2026e-02],\n",
            "        [ 8.4868e-05, -8.1239e-04,  3.8315e-03,  ...,  1.3106e-03,\n",
            "         -2.8515e-03,  1.1117e-02],\n",
            "        [ 2.7791e-03, -6.5072e-03,  1.3441e-02,  ..., -9.4073e-03,\n",
            "          8.0124e-03, -1.5341e-03],\n",
            "        ...,\n",
            "        [-3.4966e-03,  8.1531e-03, -9.3430e-03,  ...,  3.6964e-03,\n",
            "          1.2547e-02, -3.1084e-03],\n",
            "        [-8.0706e-04, -1.2329e-02, -1.1567e-02,  ...,  5.3000e-03,\n",
            "          2.7843e-04,  9.8581e-03],\n",
            "        [ 1.2708e-02,  4.2715e-04,  1.1262e-02,  ...,  1.0140e-02,\n",
            "          1.1895e-02, -4.6272e-04]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 9.4284e-03, -5.9773e-03, -8.1382e-03,  ..., -5.9969e-03,\n",
            "          6.4367e-03, -1.0359e-02],\n",
            "        [-8.8479e-03, -2.8847e-03, -4.9273e-03,  ..., -1.0937e-02,\n",
            "         -8.9397e-03, -8.5843e-03],\n",
            "        [ 8.5287e-03,  8.2279e-03, -3.1633e-03,  ..., -6.0522e-04,\n",
            "          9.1746e-03,  4.0968e-03],\n",
            "        ...,\n",
            "        [ 1.1555e-02, -8.0080e-05, -4.4054e-03,  ..., -7.1209e-03,\n",
            "          1.0538e-02,  6.0206e-03],\n",
            "        [ 4.6749e-03,  5.5103e-04,  2.8398e-03,  ..., -1.1138e-02,\n",
            "          4.5681e-03,  1.1925e-02],\n",
            "        [ 5.6814e-03,  6.7413e-03,  7.1673e-03,  ..., -3.4591e-03,\n",
            "          1.4777e-03, -1.5745e-03]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0002,  0.0117, -0.0103,  ...,  0.0018,  0.0066, -0.0123],\n",
            "        [-0.0068, -0.0082,  0.0011,  ..., -0.0108, -0.0139, -0.0042],\n",
            "        [ 0.0110,  0.0028,  0.0093,  ...,  0.0117, -0.0018,  0.0042],\n",
            "        ...,\n",
            "        [-0.0023,  0.0122,  0.0057,  ...,  0.0080, -0.0031, -0.0031],\n",
            "        [-0.0134, -0.0053, -0.0114,  ...,  0.0001, -0.0028, -0.0117],\n",
            "        [-0.0106, -0.0073,  0.0092,  ...,  0.0126,  0.0059,  0.0033]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0114,  0.0097, -0.0056,  ..., -0.0057,  0.0025, -0.0108],\n",
            "        [-0.0111,  0.0056, -0.0027,  ..., -0.0045, -0.0009,  0.0095],\n",
            "        [ 0.0077, -0.0056,  0.0050,  ..., -0.0030, -0.0034,  0.0053],\n",
            "        ...,\n",
            "        [-0.0115, -0.0112, -0.0061,  ..., -0.0044, -0.0094,  0.0052],\n",
            "        [-0.0018, -0.0098,  0.0023,  ...,  0.0045,  0.0092, -0.0093],\n",
            "        [ 0.0106,  0.0058,  0.0063,  ...,  0.0052, -0.0095, -0.0057]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.1448, -0.0079,  0.0137,  ...,  0.1183, -0.1704, -0.1064],\n",
            "        [-0.1943, -0.1223, -0.0269,  ...,  0.0420,  0.1604, -0.0749],\n",
            "        [ 0.0449, -0.1548, -0.0218,  ..., -0.0969, -0.0595, -0.1916],\n",
            "        ...,\n",
            "        [-0.0359, -0.0135, -0.0383,  ..., -0.0795,  0.0131,  0.0811],\n",
            "        [-0.1048,  0.0669, -0.0122,  ...,  0.2109, -0.0866,  0.1345],\n",
            "        [-0.1589,  0.1083,  0.1111,  ..., -0.2104, -0.0609,  0.0521]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.1189, -0.0930,  0.0147,  0.0605,  0.0994, -0.0932, -0.1142,  0.0911,\n",
            "        -0.0777, -0.0249,  0.0928,  0.1154,  0.1001,  0.0134,  0.0117,  0.0322,\n",
            "         0.1197, -0.0453,  0.0557,  0.0324, -0.0639, -0.0410,  0.0465,  0.0087,\n",
            "         0.0549,  0.0974, -0.0342,  0.1131, -0.0248, -0.0770, -0.0635,  0.0175,\n",
            "         0.0891,  0.0209,  0.0270, -0.0868,  0.0524,  0.1178,  0.0746, -0.1028,\n",
            "         0.0147, -0.0778, -0.0609, -0.0241, -0.0357,  0.0007, -0.0427,  0.0496,\n",
            "         0.1207,  0.0500,  0.1130,  0.0446,  0.0990,  0.0186,  0.0412, -0.1048,\n",
            "         0.1167, -0.0467,  0.0501,  0.0003, -0.1075,  0.0750,  0.0208, -0.0365],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0485,  0.1452, -0.1250,  ..., -0.1810, -0.0453,  0.0396],\n",
            "        [-0.1010, -0.0269, -0.2068,  ..., -0.0120, -0.1667, -0.0769],\n",
            "        [-0.0847, -0.1968,  0.0757,  ..., -0.0120,  0.0786,  0.0218],\n",
            "        ...,\n",
            "        [-0.0598,  0.0589,  0.0691,  ..., -0.1033,  0.2006, -0.1822],\n",
            "        [-0.1688, -0.0562, -0.0500,  ...,  0.1498, -0.0641, -0.1649],\n",
            "        [ 0.1174, -0.0075, -0.0745,  ..., -0.0610, -0.0231, -0.0743]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0480, -0.0649,  0.0987, -0.1017, -0.0583,  0.0878,  0.1118,  0.0498,\n",
            "         0.0988, -0.1153,  0.1013, -0.1099,  0.0927, -0.0894, -0.0745,  0.0281,\n",
            "         0.0664,  0.0875,  0.0671, -0.0770, -0.0610, -0.0853,  0.0161, -0.0256,\n",
            "         0.1186, -0.0507,  0.0017,  0.0845, -0.0280,  0.0549,  0.0982,  0.0334,\n",
            "         0.0763,  0.0403,  0.0896, -0.0369,  0.0331, -0.0827, -0.0997, -0.0574,\n",
            "        -0.0979,  0.0606, -0.0622, -0.0513, -0.0984, -0.0285,  0.0145, -0.0818,\n",
            "         0.0388,  0.0179, -0.0279,  0.0066,  0.0617,  0.0448,  0.0907,  0.0189,\n",
            "        -0.0366,  0.0493, -0.0813, -0.0910, -0.0192, -0.1055, -0.0344, -0.0599],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.2002, -0.0693, -0.1931,  ...,  0.0307, -0.0452, -0.1785],\n",
            "        [ 0.1043,  0.1105, -0.1397,  ...,  0.0613, -0.2073,  0.0771],\n",
            "        [-0.1309,  0.0707, -0.1743,  ..., -0.1126, -0.1975, -0.0692],\n",
            "        ...,\n",
            "        [-0.0924, -0.1501, -0.0246,  ..., -0.2116, -0.0163, -0.1214],\n",
            "        [ 0.0478, -0.0073, -0.1119,  ..., -0.1160, -0.1673, -0.2043],\n",
            "        [-0.0130, -0.1600,  0.1636,  ...,  0.1750,  0.1622,  0.0814]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.1019,  0.0464,  0.0576,  0.0042,  0.0061,  0.0188, -0.1155,  0.0727,\n",
            "         0.0525, -0.0680, -0.1016,  0.0551,  0.0982, -0.1185, -0.0843, -0.0568,\n",
            "         0.0410, -0.0068, -0.0940, -0.1124, -0.0511,  0.1115, -0.0193,  0.0690,\n",
            "        -0.0052, -0.1071, -0.0556,  0.0719, -0.0848,  0.0747, -0.0206, -0.0301,\n",
            "         0.1248,  0.0011,  0.0905,  0.1115, -0.0907, -0.1160, -0.0113,  0.0939,\n",
            "        -0.1166,  0.0457,  0.0407,  0.1050,  0.0424,  0.0391,  0.0531,  0.0048,\n",
            "        -0.0461, -0.0195, -0.0216, -0.0104,  0.0773,  0.0829,  0.0963, -0.0140,\n",
            "        -0.0951, -0.1157, -0.0946, -0.0219,  0.0844, -0.1078, -0.0520, -0.0345],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0938,  0.1054, -0.0055,  ..., -0.1500,  0.0819, -0.1990],\n",
            "        [-0.1187, -0.2085,  0.0873,  ...,  0.0029,  0.1913,  0.1676],\n",
            "        [-0.1377, -0.2128,  0.0271,  ..., -0.0359,  0.0494, -0.2066],\n",
            "        ...,\n",
            "        [-0.1445,  0.2153,  0.1432,  ...,  0.0475,  0.0808, -0.0615],\n",
            "        [-0.1449, -0.0921, -0.0721,  ...,  0.2158, -0.1994, -0.1649],\n",
            "        [ 0.0185,  0.0910,  0.0269,  ..., -0.1874, -0.0085,  0.0628]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0466,  0.0655, -0.1186, -0.0133,  0.0113,  0.0107,  0.1097,  0.0466,\n",
            "         0.0590, -0.0821, -0.0343, -0.0368, -0.0572,  0.0891, -0.0920,  0.1085,\n",
            "         0.0751,  0.0813,  0.0492,  0.0180, -0.1015,  0.0307, -0.0660, -0.0445,\n",
            "         0.0090,  0.0230, -0.0697, -0.0738, -0.0184,  0.0747, -0.0257, -0.0462,\n",
            "        -0.1104, -0.0156,  0.0215, -0.0242,  0.0016,  0.0171,  0.0150,  0.1022,\n",
            "        -0.1079, -0.1018, -0.0999, -0.0759, -0.0223,  0.0821, -0.0526, -0.0008,\n",
            "         0.0035,  0.0155, -0.0957, -0.0797,  0.1191,  0.1104, -0.0097,  0.0071,\n",
            "        -0.0491,  0.1214, -0.1102,  0.0489, -0.1170, -0.1237, -0.0127,  0.0478],\n",
            "       device='cuda:0', requires_grad=True)]\n",
            "Restored previous model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1orGkBxtiRY"
      },
      "source": [
        "# Optimization and evaluation\n",
        "\n",
        "Here, training and evaluation is peformed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHTQPAOcUD9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3696f9a3-f71e-42d3-ec5b-95b1c48c02eb"
      },
      "source": [
        "from time import time\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "model.train()\n",
        "n_batch = train_data.n_users // train_data.batch_size + 1\n",
        "# print(train_data.n_data)\n",
        "def compute_ndcg(top_items, test_items, test_indices, k):\n",
        "  ratings = (test_items * top_items).gather(1, test_indices)\n",
        "  norm = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
        "  dcg = (ratings / norm).sum(1)\n",
        "  dcg_max = (torch.sort(ratings, dim=1, descending=True)[0] / norm).sum(1)\n",
        "  ndcg = dcg / dcg_max\n",
        "  ndcg[torch.isnan(ndcg)] = 0\n",
        "  return ndcg\n",
        "\n",
        "def evaluate(user_embeddings, item_embeddings, k):\n",
        "  user_parts = split_mtx(user_embeddings)\n",
        "  train_parts = split_mtx(train_data.R)\n",
        "  test_parts = split_mtx(test_data.R)\n",
        "\n",
        "  recall_parts, ndcg_parts = [], []\n",
        "\n",
        "  for user_part, train_part, test_part in zip(user_parts, train_parts, test_parts):\n",
        "\n",
        "    # Get the prediction scores for the users and items as a cuda float\n",
        "    non_train_items = torch.from_numpy(1 - (train_part.todense())).float().to(device)\n",
        "    predictions = torch.mm(user_part, item_embeddings.t()) * non_train_items\n",
        "    # Get the k highest scores, scatter them as a float tensor in the GPU\n",
        "    _, test_indices = torch.topk(predictions, dim=1, k=k)\n",
        "    top_items = torch.zeros_like(predictions).float()\n",
        "    #print(test_indices)\n",
        "    top_items.scatter_(dim=1, index=test_indices, src=torch.full(test_indices.shape, 1.0).to(device))\n",
        "  \n",
        "    test_items = torch.from_numpy(test_part.todense()).float().to(device)\n",
        "    TP = (test_items * top_items).sum(1)\n",
        "    recall = TP / test_items.sum(1)\n",
        "    recall[torch.isnan(recall)] = 1\n",
        "    ndcg = compute_ndcg(top_items, test_items, test_indices, k)\n",
        "  \n",
        "    recall_parts.append(recall)\n",
        "    ndcg_parts.append(ndcg)\n",
        "\n",
        "  mean_recall = torch.cat(recall_parts).mean()\n",
        "  mean_ndcg = torch.cat(ndcg_parts).mean()\n",
        "  print('Recall:\\t' + str(mean_recall.item()))\n",
        "  print('NDCG\\t:' + str(mean_ndcg.item()))\n",
        "\n",
        "def save_state(model, optimizer, epoch):\n",
        "  torch.save(model.state_dict(), models_path/'model1.pt')\n",
        "  torch.save(optimizer.state_dict(), models_path/'optimizer.pt')\n",
        "  torch.save(torch.IntTensor(epoch), models_path/'epoch.pt')\n",
        "\n",
        "def train(model, data, t):\n",
        "  total_loss = 0\n",
        "  start = time()\n",
        "  timings = []\n",
        "  for b in range(n_batch):\n",
        "    \n",
        "    batch_start = time()\n",
        "    u, i, j = data.sample() #u是一个batch中的1024个users id， i是1024个positive sample, j是1024个negative sample\n",
        "    u = torch.from_numpy(u).long().to(device)\n",
        "    i = torch.LongTensor(i).to(device)\n",
        "    j = torch.LongTensor(j).to(device)\n",
        "    # print(\"\\n u\", u)\n",
        "    # print(\"\\n i\", i)\n",
        "    # print(\"\\n j\", j)\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(u, i, j) #forward函数执行\n",
        "    #print(\"loss\", loss.item())\n",
        "    loss.backward() #对loss中的每一个变量求偏导\n",
        "    optimizer.step() #更新参数 W1(8*8) b1(8*8), W2(8*8) b2(8*8) 3层layers\n",
        "    total_loss += loss.item() #把除了loss.backward()之外的对loss的调用都改成loss.item(), 否则loss会一直迭代\n",
        "    timings.append(time()-batch_start) #计时\n",
        "\n",
        "  avg_batch = np.average(timings)\n",
        "  print('Finished epoch ' + str(t+1) + ' in\\t' + str(time()-start) + ' sec')\n",
        "  print('Total BPR loss:\\t\\t' + str(total_loss))\n",
        "  print('Average batch time:\\t' + str(avg_batch))\n",
        "\n",
        "for t in range(n_epochs):\n",
        "  print('Starting epoch: ' + str(t+1))\n",
        "  train(model, train_data, t)\n",
        "  save_state(model, optimizer, t)\n",
        "  if (t+1) % 5 == 0:\n",
        "    model.eval()\n",
        "    evaluate(model.user_embeddings_final, model.item_embeddings_final, k=10)\n",
        "    model.train()\n",
        "  print('\\n============\\n')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch: 1\n",
            "Finished epoch 1 in\t54.819281578063965 sec\n",
            "Total BPR loss:\t\t1.5745480693876743\n",
            "Average batch time:\t1.827303139368693\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 2\n",
            "Finished epoch 2 in\t54.735039710998535 sec\n",
            "Total BPR loss:\t\t1.5858487114310265\n",
            "Average batch time:\t1.8244951963424683\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 3\n",
            "Finished epoch 3 in\t54.73480987548828 sec\n",
            "Total BPR loss:\t\t1.5431170463562012\n",
            "Average batch time:\t1.8244882424672444\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 4\n",
            "Finished epoch 4 in\t54.473047494888306 sec\n",
            "Total BPR loss:\t\t1.6059352047741413\n",
            "Average batch time:\t1.8157624165217081\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 5\n",
            "Finished epoch 5 in\t54.66131615638733 sec\n",
            "Total BPR loss:\t\t1.5387996658682823\n",
            "Average batch time:\t1.822038507461548\n",
            "Recall:\t0.06492892652750015\n",
            "NDCG\t:0.18594184517860413\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 6\n",
            "Finished epoch 6 in\t55.036553144454956 sec\n",
            "Total BPR loss:\t\t1.5713114701211452\n",
            "Average batch time:\t1.8345462083816528\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 7\n",
            "Finished epoch 7 in\t54.921457290649414 sec\n",
            "Total BPR loss:\t\t1.4578590467572212\n",
            "Average batch time:\t1.8307097911834718\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 8\n",
            "Finished epoch 8 in\t55.01981496810913 sec\n",
            "Total BPR loss:\t\t1.580365177243948\n",
            "Average batch time:\t1.8339882055918375\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 9\n",
            "Finished epoch 9 in\t54.75603699684143 sec\n",
            "Total BPR loss:\t\t1.4836680553853512\n",
            "Average batch time:\t1.82519580523173\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 10\n",
            "Finished epoch 10 in\t54.87635397911072 sec\n",
            "Total BPR loss:\t\t1.5154429972171783\n",
            "Average batch time:\t1.8292054732640584\n",
            "Recall:\t0.06403599679470062\n",
            "NDCG\t:0.18271750211715698\n",
            "\n",
            "============\n",
            "\n"
          ]
        }
      ]
    }
  ]
}